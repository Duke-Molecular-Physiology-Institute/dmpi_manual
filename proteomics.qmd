# Proteomics

## Introduction {#sec-intro}

This section template is designed to give a step-by-step walk-through of the data analysis performed in @lyons_proteomics_2022, which served as a companion manuscript to @wilson_disruption_2022. The goal of the analysis was to determine the effects of STIM1 knockout (KO) on protein abundance and sites of protein phosphorylation in gastrocnemius muscle tissue using data from large-scale proteomics and phosphoproteomics experiments. The data considered here consist of protein abundance values obtained from five STIM<sup>fl/fl</sup> mice and five STIM<sup>-/-</sup> mice. The code used in this section is based on the R code accompanying the @lyons_proteomics_2022 article.

## Before analysis {#sec-before}

Depending on the nature of your experiment, you may be ready to analyze your data as soon as it comes off the machine, or there may be several steps of cleaning and pre-processing before it makes sense to draw conclusions from your data. This tutorial organizes the most common steps before data analysis into three categories: 1. loading the data, 2. filtering the data, and 3. normalizing the data.

### Loading the data into R

Before working with data in R, it needs to be loaded from the raw files that come off the machine (or other files that you have created) into R objects. In this tutorial we will be working with plain text files, which can be read with the R package `readr`. The package `readr` is part of a host of packages included in `tidyverse`, so we will load that, along with other packages, which will help us write concise code. Online documentation for all of these packages can be found in @sec-rdocs.

```{r, message=F, warning=F}
#
library(tidyverse)
library(janitor)
library(magrittr)

```

Since the files we are working with here have a `.txt` extension and are tab-separated, we can use the `read_tsv` command in the `readr` package to load them into R:

```{r, message=F, warning=F}
# load mito_carta file for mitochondria data
mito_carta = readr::read_tsv("data/Mouse.MitoCarta3.0.txt")
colnames(mito_carta)[[1]] = "GeneName"
  
# load the peptide and protein data
peptides = readr::read_tsv("data/Muoio_BeckyWilson_STIM1-SKM_TMT10_FINAL_2020-05-03_SN2pt5_PeptideIsoforms.txt")
proteins = readr::read_tsv("data/Muoio_BeckyWilson_STIM1-SKM_TMT10_FINAL_2020-05-03_SN2pt5_Proteins.txt")

# create variables for the samples being compared and the total number of samples
ID_names = c("WT", "KO", "pool")
total_number_of_samples = 11
```

Note that the `readr::read_tsv` syntax above is overkill; once we load the `tidyverse` library, we can use functions within an R package without specifying the package name. However, in a tutorial like this, writing out the package name can help with understanding where each function comes from. Doing so also provides a visual cue that we are using a function from an R package.

Once you have loaded your data into an R object, it is always a good idea to visually inspect the R object to determine whether the data have been loaded correctly. You can preview tabular R objects, like the `proteins` object above, using the `View()` command as follows:

```{r, eval=F}
View(proteins)
```

### Filtering the data

Filtering data is the process of discarding pieces of a dataset that will not be used in subsequent normalization or analysis steps. It is a conceptually trivial step that can, in practice, take a fair amount of care, consideration, and time. The basic principle is that if some part of your primary dataset will not be used in subsequent steps, it is a good idea to trim this part from the R objects you are using to store your data.

In our example, we will first filter our data by extracting the columns we need. We'll store these names in an R object:

```{r}
#
columns_to_extract = list(cols_protein = janitor::make_clean_names(c("Description", "Accession", "Master", "Exp. q-value: Combined",
                                     "# Peptides", "# PSMs", "# Protein Unique Peptides", "# Unique Peptides", "Entrez Gene ID",
                                      "Reactome Pathways", "WikiPathways")),
                          cols_peptide = janitor::make_clean_names(c("Master Protein Accessions", "Protein Accessions", "Sequence",
                                             "# Missed Cleavages", "PSM Ambiguity", "# PSMs", 
                                             "Modifications", "Modifications in Proteins", "XCorr (by Search Engine): Sequest HT",
                                             "Deltam/z [Da] (by Search Engine): Sequest HT"
                                             )))
```

Then we will filter the protein and peptide R objects so that we remove the columns we don't need:

```{r}
#
filtered_proteins = proteins %>%
                    janitor::clean_names() %>%
                    dplyr::select(., dplyr::all_of(columns_to_extract[["cols_protein"]]), dplyr::starts_with("abundance_F2_"))

#
filtered_peptides = peptides %>%
                    janitor::clean_names() %>%
                    dplyr::select(., dplyr::all_of(columns_to_extract[["cols_peptide"]]), dplyr::starts_with("abundance_F6_"))
```

Next, we will filter the protein and peptide R objects so that we remove the rows we don't need:

```{r}
#
filtered_proteins = filtered_proteins %>%
                    dplyr::mutate(na_row_count = rowSums(is.na(select(., dplyr::starts_with("abundance"))))) %>%
                    dplyr::filter(na_row_count <= 5) %>%
                    dplyr::filter(master == "IsMasterProtein" & exp_q_value_combined < 0.01)

#
filtered_peptides = filtered_peptides %>%
                    dplyr::mutate(na_row_count = rowSums(is.na(dplyr::select(., dplyr::starts_with("abundance"))))) %>%
                    dplyr::filter(na_row_count <= 5)  %>%
                    dplyr::filter(., grepl('Phospho', modifications))
```

### Normalizing the data

Some form of normalization is frequently required before drawing conclusions from 'omics datasets. There are two primary reasons for this: 

1. **Batch effects:** The data that were recorded may have been subject to a systematic artifact that can be detected and potentially adjusted for.
2. **Measurement scaling**: The raw values that are measured by the 'omics technology do not correspond directly to the underlying value that we are interested in. For instance, measurement devices may measure absolute abundance of a biological product such as a protein, transcript, or metabolite, whereas we may be interested in the *relative* abundance of that product. The absolute value may be a function of the experimental protocol, rather than a function of the relevant biology.

In this tutorial, we will apply a simple normalization to address slight variations in total sample "loading" (i.e. $\mu \text{g}$ of peptide labeled). This variation can be seen in the plot below, where we show the total abundance (the sum of abundance across all proteins) for each mouse:

```{r}
#
filtered_proteins %>%
  dplyr::select(dplyr::starts_with("abundance_")) %>%
  magrittr::set_colnames(paste0("Mouse ", 1:11)) %>%
  colSums() %>%
  barplot(ylab = "Total abundance")
```

While we are certainly interested in comparing the difference in abundance for individual proteins between the KO and WT mice, we do not have any a priori reason to believe that the *total* abundance should vary between mice. To normalize, we will simply divide each mouse's abundance levels by a ratio so that the total abundance is the same across mice.

```{r}
#
normalized_proteins = filtered_proteins %>%
                      dplyr::mutate_at(dplyr::vars(dplyr::starts_with("abundance_")), function(x){x / sum(x)})
```

## Exploratory Data Analysis {#sec-eda}

## Statistical testing {#sec-stattest}

In this tutorial, we are interested in testing the difference in protein abundance between WT and STIM1 KO mice. The two types of mice represent two distinct populations, and the specific mice used in our experiment are a small sample from these populations. For a given protein---let's call it protein $j$---we would like to know whether the population average abundance among WT mice is the same or different than the population average abundance among STIM1 KO mice. We can resolve this question by testing the null hypothesis that says the average abundances are the same:
$$
H_0: m^{\text{WT}}_j = m^{\text{KO}}_j,
$$
versus the alternative hypothesis, which says they are different:
$$
H_1: m^{\text{WT}}_j \neq m^{\text{KO}}_j.
$$

We will use a version of the two-sample $t$-test as implemented in the R package `limma` (see @sec-rdocs for info) to decide this question for each protein. The classical t-statistic for the $j^\text{th}$ protein is computed as
$$
t_j = \frac{\hat{m}_j^\text{KO} - \hat{m}_j^\text{WT}}{\hat{\sigma}_j},
$$
where $\hat{m}_j^\text{KO}, \hat{m}_j^\text{WT}$ are the mean abundance for protein $j$ in the KO and WT groups, respectively, and $\hat{\sigma}_j$ is the pooled standard deviation of the abundance measurements for protein $j$. Here "pooled" refers to the fact that it is usually assumed that the variances of the KO and WT measurements are the same, so we can combine the data from both groups to estimate the measurement variability.

The $t$ statistic used in the `limma` package is close to the classical one, but it uses a different estimate of the measurement variability, which "shrinks" each $\hat{\sigma}_j$ towards an average value $\bar{\sigma}$ that is estimated from the entire dataset (i.e. all proteins). The motivation behind using this modification to the classical procedure is that estimates of measurement variability can be poor if the sample size is small. In this tutorial, there are only 5 mice in each of the KO and WT samples, so indeed the sample standard deviation is not a very precise estimate of the true measurement variability. By shrinking towards the overall average standard deviation $\bar{\sigma}$, the estimate used by the `limma` package is reducing the imprecision of $\hat{\sigma}_j$. This type of approach to estimation and hypothesis testing falls under the umbrella of "empirical Bayes."

## Statistical language {#sec-statlang}

Tests for differential protein abundance were performed using the `lmFit` and `eBayes` functions in the R package `limma` [@ritchie_limma_2015; @phipson_robust_2016]. Statistical significance for each test was assessed with an adjusted p-value, computed from the raw p-values using the Benjamini-Hochberg procedure [@benjamini_controlling_1995]. 

